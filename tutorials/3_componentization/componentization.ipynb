{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentization\n",
    "\n",
    "One of the points in the mission of kubeflow is:\n",
    "\n",
    "* Easy, repeatable, portable deployments on a diverse infrastructure (for example, experimenting on a laptop, then moving to an on-premises cluster or to the cloud)\n",
    "\n",
    "In this tutorial I'll demonstrate how we can containerize some of our functionalities and turn them into components for use in different projects. Using containerized pieces of software has many benefits, some of which I'll highlight here:\n",
    "\n",
    "* A containerized piece of code can run anywhere; locally, on a VM on-prem, but also in the cloud on kubernetes.\n",
    "* The application code doesn't need to be provisioned on the VM anymore, leading to less likelihood of outages. All the code and dependencies are in the container, so the code is ready to go.\n",
    "* Thinking in terms of pieces of code doing specific things in a pipeline is helpful, it avoids building large monoliths that take ages to test and develop before you get to the critical section.\n",
    "* Kubeflow components typically pick up artifacts from a file on a (shared) disk. So also locally, there are less dependencies with remote services, databases for extraction, so it's easier to run automated system tests over the whole application.\n",
    "* Segregating functionality through containerization can make it easier for teams to work together on the same project, because you're not all touching the same code and design.\n",
    "* If something has to be productionized fast for further evaluation and it's in an exotic language, you don't have to rewrite everything in the code you understand, you can containerize the exotic language and start running the pipeline.\n",
    "* If a component is generic enough, it can be reused very easily in another pipeline (because it's in your Container Registry).\n",
    "* When you use packages/libraries, there are cases where for one part of the application version X is needed and for another version Y, because you use a package Z that has different dependencies. Splitting the functionalities up clears up a lot of these dependencies.\n",
    "* Having the code in the container means that when you deploy your app, you don't continuously query package servers. Some of those package servers may not be owned by you and this could lead to problems in deployment.\n",
    "* Containerization by itself doesn't require specific incompatible changes to the software itself. You can still deploy \"old style\" on other VM's that need the same package.\n",
    "\n",
    "So those are the positive points for containerization. Kubeflow has a very reasonable documentation on componentization, available here: \n",
    "\n",
    "https://www.kubeflow.org/docs/pipelines/sdk/component-development/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op, InputPath, OutputPath\n",
    "\n",
    "host='http://localhost:8080'\n",
    "client = kfp.Client(host=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build two components\n",
    "\n",
    "We'll build two components in this example pipeline. One simulates getting the data from an external source to not have to rely on any service for the example. The other is a component that preprocesses that data. Componentization is about this:\n",
    "\n",
    "* Containerization of the script/application that you want to run, which you do through a Dockerfile.\n",
    "* A component description (yaml) that is used by the pipeline compiler at build time. You do not need that file at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source component\n",
    "\n",
    "The data source component is available under src/components/datasource. A component consists of the following:\n",
    "\n",
    "* The binary or script code for the application, baked into a docker container.\n",
    "* A Dockerfile describing how to dockerize the application\n",
    "* A component file for kubeflow, describing the input and output parameters.\n",
    "\n",
    "Every component is written as a command line application. When the component needs to read input data, it typically reads that from a local file on a shared disk. Kubeflow will generate and manage the locations of the output path. An output path can be \"attached\" to the input path of another component.\n",
    "\n",
    "Obviously, you can also pass cloud storage locations (GCS or S3) or add queries to be executed on databases or BigQuery, where the docker container then becomes responsible for writing the query results to the output file.\n",
    "\n",
    "For the data source component, we don't have any input arguments for getting the data from some other source, we only have an outputPath argument where we write the data file too. The data file, for the purpose of this tutorial, is baked inside the container to make it easy to write this out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Component specification**\n",
    "\n",
    "Let's start with a component specification. This specification helps kubeflow to determine which parameters it can manage, what the acceptable interface is towards the application within the container (the container is an opaque processing element otherwise, where no one knows if the parameters make sense or not). First, the component yaml describes the parameters that the container accepts. Then you see those repeated, but that describes how the parameters from the kubeflow pipeline are applied to the arguments of the application.\n",
    "\n",
    "Here is the component specification for the datasource, with a lot of comments explaining how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# This part is the component description, describing the interface\n",
    "# for the application.\n",
    "name: datasource\n",
    "description: Generates some data for us to work with.\n",
    "inputs:\n",
    "    # There are no inputs for this application\n",
    "outputs:\n",
    "  - name: Output Path   # This is the name of the output\n",
    "    type: OutputPath    # The type determines how it is applied in kubeflow.\n",
    "                        # Simple types are simply passed, InputPath and OutputPath are more managed.\n",
    "implementation:\n",
    "  # The implementation part describes how the inputs/outputs described above are\n",
    "  # applied to the \"implementation\". So you'll see the names repeated, but that makes sense.\n",
    "  # It decouples the \"component interface\" part from the \"how to run with those parameters\" part.\n",
    "  container:\n",
    "    image: datasource:latest\n",
    "    command: [\n",
    "      python3,  # The interpreter that we need\n",
    "      \"/components/extract_data.py\",  # The path to the application.\n",
    "      \"--output-path\",  # The name of the command line argument (see argumentparser)\n",
    "      {outputPath: Output y URI}, # Substituted by kubeflow with the value of \"Output Path\" passed\n",
    "                                  # into the component\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing component\n",
    "\n",
    "The preprocessing component has two parameters. One is the InputPath, which is connected to the OutputPath of the datasource component. The other is another OutputPath, which will contain the data file after conversion. This file also has a component yaml file, a Dockerfile, a build script and a very little amount of source code.\n",
    "\n",
    "The component description for preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# This part is the component description, describing the interface\n",
    "# for the application.\n",
    "name: preprocess\n",
    "description: Preprocesses the data from the first stage\n",
    "inputs:\n",
    "  - name: Input Path\n",
    "outputs:\n",
    "  - name: Output Path\n",
    "    type: OutputPath    # The preprocessing app generates a new output file.\n",
    "                        # But the location where is managed by kubeflow.\n",
    "implementation:\n",
    "  container:\n",
    "    image: localhost:5000/preprocess:latest\n",
    "    command: [\n",
    "      python3,\n",
    "      \"/component/preprocess.py\",\n",
    "      \"--input-path\",\n",
    "      {inputPath: Input Path},  # Substituted by kubeflow with \"Input Path\"\n",
    "      \"--output-path\",\n",
    "      {outputPath: Output Path}, # Output path generated by kubeflow\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the pipeline\n",
    "\n",
    "With the components designed and figured out, we can design the actual pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op, InputPath, OutputPath\n",
    "\n",
    "# kfp.components has alternative methods for loading from a url location as well\n",
    "datasource = kfp.components.load_component_from_file('components/datasource/component.yaml')\n",
    "preprocess = kfp.components.load_component_from_file('components/preprocessor/component.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Componentization',\n",
    "    description='Using components as part of the pipeline'\n",
    ")\n",
    "def components_pipeline():\n",
    "    datasource_task = datasource()\n",
    "    preprocess_task = preprocess(input_path=datasource_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(components_pipeline, 'components_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://localhost:8080/#/experiments/details/b123554a-9d1e-4d7c-9a95-dd00925d8e25\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://localhost:8080/#/runs/details/ba27e2be-2312-46f3-ac54-97d532014a53\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=ba27e2be-2312-46f3-ac54-97d532014a53)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(components_pipeline, arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
