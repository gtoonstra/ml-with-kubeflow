{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows you in a very simple way how you can start writing python code that executes on the notebook server and then turns that into a kubeflow pipeline and componentizing them. \n",
    "\n",
    "To start off, let's just print \"hello world\" in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print this differently using a shell command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "!echo \"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to automate things on kubeflow, we need know how to connect to the kubeflow server and execute actions there. There's an SDK for Kubeflow called \"kfp\", which contains a client object that can execute a number of actions. When you run python notebooks on the kubeflow server itself, you only have to import kfp and it will connect to the cluster it runs on. When you do not run on a notebook pod, which is the case in either AI Platform Notebooks or a local deployment, you should specify the hostname to connect to separately, as I've done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "host='http://localhost:8080'\n",
    "client = kfp.Client(host=host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiments': [{'created_at': datetime.datetime(2020, 9, 24, 11, 30, 17, tzinfo=tzutc()),\n",
       "                  'description': 'All runs created without specifying an '\n",
       "                                 'experiment will be grouped here.',\n",
       "                  'id': '4f2c1759-d018-4e9f-b893-b4043a821413',\n",
       "                  'name': 'Default',\n",
       "                  'resource_references': None,\n",
       "                  'storage_state': 'STORAGESTATE_AVAILABLE'}],\n",
       " 'next_page_token': None,\n",
       " 'total_size': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first pipeline we'll turn the hello world statement into a function and provide the minimal amount of code to run that on kubeflow. Thus, we demonstrate how to build a pipeline, but also give you the information that it is possible to run a python function directly on kubeflow with one simple decorator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import gives us access to the domain specific language to create pipelines\n",
    "from kfp import dsl\n",
    "# this import is to be able to turn a python function into a containerOp (a kubeflow task).\n",
    "from kfp.components import func_to_container_op\n",
    "\n",
    "\n",
    "@func_to_container_op\n",
    "def hello_world():\n",
    "    print(\"Hello world\")\n",
    "\n",
    "\n",
    "# This decorator specifies that this function defines the pipeline. The function can have a number of arguments,\n",
    "# all of which will be added to the UI of kubeflow as well, so that you can specify the value of those arguments in the UI.\n",
    "# In this case no arguments are given for simplicity.\n",
    "@dsl.pipeline(\n",
    "    name='Hello World pipeline',  # The name for the pipeline\n",
    "    description='Just prints \"Hello World\".'  # The description of the pipeline\n",
    ")\n",
    "def hello_world_pipeline():\n",
    "    # Here it defines one task, composed of our python function.\n",
    "    echo_task = hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pressing enter, nothing happens. This is because we've only defined what the pipeline should look like in python code, but we didn't specify anything with regards to what to do with it. One of the things we can do is compile the pipeline into YAML, which we can then upload to Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(hello_world_pipeline, 'hello_world_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the browser on the left you would now see the yaml file appear. This YAML file is what kubeflow uses to define the workflow. Open it to see what's in there and see if you recognize all the elements from the pipeline.\n",
    "\n",
    "Another thing we can do with the SDK is to trigger this pipeline on kubeflow, saving the step of manually uploading it and triggering it from the UI. When we do that, the experiment link and the run link appear for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://localhost:8080/#/experiments/details/4f2c1759-d018-4e9f-b893-b4043a821413\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://localhost:8080/#/runs/details/79444b0d-3b71-4238-b239-7bbd4124c007\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=79444b0d-3b71-4238-b239-7bbd4124c007)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(hello_world_pipeline, arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, familiarize yourself with the UI, where to find the task logs, the task output. Most of it is self explanatory.  You'll notice that even though we triggered a pipeline run on kubeflow, the pipeline definition is not available in the \"Pipelines\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including a container more explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Kubeflow, anything that runs is always a container, even if you just run a python function as above (check the yaml to figure out which docker container it used). We can pass parameters to containers. Even for as simple as printing a \"Hello World\" message, we must download a container that can run the command. In this case we will use the `library/bash:4.4.23` container, which then runs one shell command to echo hello world. Here is what the pipeline then looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of the function decorator, we make an explicit reference to a container image using the \"kfp.dsl\" module.\n",
    "def echo_op():\n",
    "    return dsl.ContainerOp(\n",
    "        name='echo',\n",
    "        image='library/bash:4.4.23',\n",
    "        command=['sh', '-c'],\n",
    "        arguments=['echo \"hello world\"']\n",
    "    )\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Hello World pipeline 2',\n",
    "    description='Just prints \"Hello World\" through the shell.'\n",
    ")\n",
    "def hello_world_pipeline_2():\n",
    "    echo_task = echo_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://localhost:8080/#/experiments/details/4f2c1759-d018-4e9f-b893-b4043a821413\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://localhost:8080/#/runs/details/cf73cb18-7d04-4349-aacb-fa227d5a736f\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=cf73cb18-7d04-4349-aacb-fa227d5a736f)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compilation is not necessary for running this on kubeflow, just to generate the YAML so you can see what's in there.\n",
    "kfp.compiler.Compiler().compile(hello_world_pipeline_2, 'hello_world_pipeline_2.yaml')\n",
    "client.create_run_from_pipeline_func(hello_world_pipeline_2, arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll revisit the example and add some pipeline parameters. I advise always to use defaults, so that the pipeline is triggerable without overridden parameters and doesn't crash. Also in the UI, the default parameters prepopulate the input boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding one parameter to this step. This is typically how you pass parameters into containers\n",
    "def echo_op_with_params(hello_to_who):\n",
    "    return dsl.ContainerOp(\n",
    "        name='echo',\n",
    "        image='library/bash:4.4.23',\n",
    "        command=['sh', '-c'],\n",
    "        arguments=[f'echo \"hello {hello_to_who}\"']\n",
    "    )\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Parameters pipeline',\n",
    "    description='Adds parameters to the basic pipeline.'\n",
    ")\n",
    "def hello_world_pipeline_3(hello_to_who=\"Sherlock Holmes\"):\n",
    "    # Passing the parameter to the step\n",
    "    echo_task = echo_op_with_params(hello_to_who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"http://localhost:8080/#/experiments/details/4f2c1759-d018-4e9f-b893-b4043a821413\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://localhost:8080/#/runs/details/51277e5d-1e8e-4c41-adc2-43bfe5437d94\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=51277e5d-1e8e-4c41-adc2-43bfe5437d94)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(hello_world_pipeline_3, arguments={\"hello_to_who\": \"Watson\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is also important to demonstrate how we can upload pipelines to kubeflow. As you may have noticed, the pipelines before executed fine on kubeflow, but in the \"Pipelines\" section, the definition of the pipeline is not available for a manual trigger for example. For that to happen, we compile the pipeline first and we'll upload the YAML to kubeflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(hello_world_pipeline_3, 'hello_world_pipeline_3.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the ```hello_world_pipeline_3.yaml``` file to kubeflow (from the \"Pipelines\" section, the \"Upload\" button top right). After it is uploaded, you can do a \"Create Run\" for it, which will trigger the pipeline with the supplied parameters. Have a look around the UI to familiarize yourself further.\n",
    "\n",
    "Play around a bit more, familiarize more, you now know the basis of kubeflow pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of first tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
